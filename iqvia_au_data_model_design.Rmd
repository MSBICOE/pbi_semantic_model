---
title: "IQVIA AU Analytical Semantic Data Model Design Consideration"
description: |
  The guide for IQVIA AU unified data model design
author:
  - name: Steven Wang
    affiliation: AU Data Science
  - name: Jun Bai
    affiliation: AU Data Science
date: "`r Sys.Date()`"
output: 
  distill::distill_article:
    toc: true
    self_contained: yes
      
---

<style type="text/css">

  h2 {
  font-size: 12px;
  font-weight: normal
  font-family: Sans-serif;
  }

</style>


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
library(kableExtra)
library(formattable)
### SQL connection
library(odbc)
drv <- odbc::odbc()
connection <-  paste0("Driver={SQL Server Native Client 11.0};",    # Drivers
                      "Server=sydssql162d;", # Server Address
                      "Database=Product_Customization;", # Database Name
                      "Trusted_Connection=yes;")

con_odbc <- dbConnect(drv,
                      .connection_string = connection, encoding = "latin1")

connection_iqvia <-  paste0("Driver={SQL Server Native Client 11.0};",    # Drivers
                      "Server=SQLETL02D.INT.MIPORTAL.COM;", # Server Address
                      "Database=iqviaau_reporting_db;", # Database Name
                      "Trusted_Connection=yes;")

con_odbc_iqvia <- dbConnect(drv,
                      .connection_string = connection_iqvia, encoding = "latin1")

```

# Introduction

In order to meet the increasingly internal and external demanding on the self-analytic capability, AU DS team is assigned to design a semantic data model to meet this requirement.

While the initial request is based on the Bayer Consumer health sub-state data requirements, the design concept is aiming for both internal and all external client use. With this in mind, an overarching consideration of the model to meet all needs is enforced in the designing stage.


# Conceptual Data Modelling

Given that many clients share same IQVIA product master table and same retail pharmacy stores, a single physical source analytic data warehouse will be adopted instead of each client having its own database.

### The Data Model Overall Usage Flow 

Only one centralized analytic data mart will be created. A set of identical views will be used for each different client which only relevant to the client's subscription.

A standardized yet also customizable parameter-driven Power BI data model will be designed as template. 

Each client will use its views to feed into the Power BI data model to create its own Power BI workbook.

Each client's Power BI workbook will be published to its own Power BI Premium workspace.

```{r modelflow, fig.cap="Data model usage flowchart", fig.height=8, fig.width=8}
library(DiagrammeR)
mermaid("
    graph TB
    C(Centrlized Single DB)-- Create -->V1[Client 1 Views]
    C-- Create -->V2[Client 2 Views]
    C-- Create -->V4[IQVIA Views]
    V1-- Feed Data -->P(PBI Master Model)
    V2-- Feed Data -->P
    V4-- Feed Data -->P
    P-- Apply Model -->M1[Client 1 PBI]
    P-- Apply Model -->M2[Client 2 PBI]
    P-- Apply Model -->M4[IQVIA PBI]
    M1-- Publish to -->W1[Client 1 Workspace]
    M2-- Publish to -->W2[Client 2 Workspace]
    M4-- Publish to -->W4[IQVIA Workspace]
    W1-- Embedded in -->Mi(MiPortal)
    W2-- Embedded in -->Mi
    W4-- Embedded in -->Mi
    W1-. Analysing in .->U(User Tools On Premise)
    W2-. Analysing in .->U
    W4-. Analysing in .->U
    style C fill:#00b300,stroke:#333,stroke-width:4px;
    style Mi fill:#00b300,stroke:#333,stroke-width:4px;
    style P fill:#1aff1a,stroke:#333,stroke-width:2px;
    style V4 fill:#E59925;
    style M4 fill:#E59925;
    style W4 fill:#E59925;
    style U fill:#F1EA0E,stroke:#333,stroke-width:4px;
    ", height = 500, width = 700)
```


### Data Model Conceptual ERD

The conceptual ERD for the data model is proposed as the below Figure \@ref(fig:dmodel)

```{r dmodel, fig.cap="Data model design conceptual ERD"}

knitr::include_graphics("image/pbi_erd_banner_state_7.svg")

```

As Figure \@ref(fig:dmodel) shows, the model is mainly a star schema design. At brick level, we only have 1 fact table to hold the fact data. Since the fact table needs to accommodate not only the retail pharmacy data but also the Grocery and other datasets. This simplified fact table design will make some burden at the ETL side. Particularly, when some data sources which don't have same granular of data at brick or banner level, we will need a way to nominate a brick or banner to hold the upper level fact data.


Even though that a client is not supposed to view the store level data, a store level fact table is indeed also designed for 3 purpose:

1) to be used for the dynamic store distribution calculation;
2) to be made available for internal users;
3) When a client is subscribed to the probe data for Sellin;

There is no additional fact table to hold the CBG data. CBG metrics will be controlled by DAX by using context understanding the geographic level and banner selection.

# Reference Table Considerations

## IQVIA Product Master Dimension Table

IQVIA AU product master table which sourced from `warehousedata_pos_dim_product_parquet` table from BDF has more than 170 attributes. Many attributes in this master table are not interested by AU clients, particularly when it is consumer health concerned. Since the current main focus of this project is consumer healthy, most ethical related attributes are not considered for importing into the model.

### IQVIA Product Master Further Cleaning Up

The product master from BDF has still few attributes conflicting and some fundamental key indicator has not presented, like there is no simple flag to filter out the consumer health products.

In addition, that at the time of this writing which is 10th July 2020, there roughly about 4500 consumer products which have NEC classification but they don't have CHC classification. In contrast, there are also about 4500 products which have CHC code but it don't have NEC codes.

For the light PFC products, due to the nature of meaning of light PFC, the vast majority of attributes are not populated. However, some attributes which are indeed available based on other available information which are also not populated, like 

While for the master table, ideally all the changes are better fixed at the source, for the completeness of the data modelling, DS team has done some further clean-up for the master product table.

1. For the light PFC products, based on the field `pharml_mfr_co_lng_nm`, we derived and filled 2 more columns: `pharml_mfr_co_abrv_cd`, and `pharml_mfr_co_short_nm`

2. For those `pharml_mfr_co_lng_nm` not currently available in normal PFC, we set the `pharml_mfr_co_short_nm` same as `pharml_mfr_co_lng_nm`. Meanwhile, for the `pharml_mfr_co_abrv_cd` we added dummy code starting from Z001.

3. There are a large number of product which are consumer health products but they don’t have a CHC code related and currently in the BDF it leaves it blank. We used some logic to deduce the CHC code based on the NEC4 description and repopulated this blank chc code with our deduced values. The same is for those products which have CHC code but they don't have an NEC code. In order to distinguish it is DS deduced CHC value, we added this flag: `DS_Deduced`, and if the value is ‘y’ it is deduced.
    + We know this is a very coarse logic for such large volume of SKUs and some SKUs may not have any sales. For the purpose of data completeness, we coded those SKUs regardless it has sales or not. However, if production can use the same logic to review the coding and correct them then the coding time can be largely saved.

4. It is frustrating that a product master table doesn’t have a clear flag which can get just Consumer health products and the product master table attributes are quite esoteric for many end users. For the simple to filter out the consumer health product, we added this flag: `CHC_flag`, if it is ‘y’ then it is consumer health product.

5. Light PFC caused a huge issue in the reporting database side, as the vast majority of them even it is coded but it is indeed not valuable for the reporting purpose. We created a Flag: `NEC_Other_no_CHC`, for those product which may not that useful, we may can group them together at the manufacturer level while reporting.

6. Should we have a flag for those LightPFCs which need to be reported as clients have identified them? If this is the case, then we can group the lightPFC products to its manufacturer and/or CHC code if they have. This can be largely reduce the product master table and fact table size. There are 490K PFCs at this moment with LightPFC. Meanwhile, it has 110K products if we don't count LightPFC. If we can group some lightPFC together for the revenue completeness purpose, then the data size can be largely reduced


### IQVIA Product Master Attributes Used

Not all IQVIA product master attributes will be brought into this model, we currently only bring in the attributes most relevant to consumer health related. This can be expanded in the future if needed. For this exercise, we indeed linked CWH rate category into the master table. This can be done at BDF side in the future. Table \@ref(tab:piqvia) shows attributes we will use for our IQVIA product master in the data model and its proper descriptive names.

```{sql piqvia, connection = con_odbc, echo = FALSE, max.print = -1, tab.cap = "Product IQVIA Master View Column Definition"}

SELECT [attribute_name]
      ,[attribute_name_use] As attribute_name_display
      ,IsNUll([description], '') As description
FROM [dbo].[pbi_Product_Attributes_IQVIA];

```

## Client Product Master Dimension Table

IQVIA has a client territory and product segmentation customization tool: Everest, this tool has given the client great flexibility to customize the product attributes associated with each different product category. In contrast, this flexibility also caused some issue for the downwards reporting side maintenance issue.

For this semantic data model, each client has its own client product dimension table. For the model standardization point of View, the model will use a fixed number of standardized client product attributes. In order to standardize the client product table attributes, DS team has done such:

1) Get all currently available client product attributes;

2) Use fuzzy grouping to map the similar attributes to a master attributes name;

3) Save this raw attributes name to master attributes name to a master attributes lookup table. 
    + When the new attributes added in client attributes, this table will get updated.

4) For each client, each category it will have its own sets of product attributes, some are the same and some are not.
    + DS will create a unified client product table: ClientName_AllCategories to accommodate all categories
    + If some attributes is not available for some categories, then NULL value will be used.

5) DS team has created an process to create the client product master view.
    + All client product master view will have the exactly same attributes
    + Underneath the display attribute names, each attribute is mapped to its proper client product attributes used via attributes mapping table
    + In the model side, only the attributes which are relevant to a client are displayed

The Table \@ref(tab:pclient) below depicts client attributes used for client product master and the standardized view column definition. The SQL code shows how this view columns are generated.

```{sql pclient, connection = con_odbc, echo = TRUE, max.print = -1, tab.cap = "Product Client View Column Definition"}

with cte
as
(
  Select Distinct B.Name
  From 
  sys.tables A
	Inner Join
  sys.all_columns B
	On A.object_id = B.object_id
  Where A.name = 'Blackmores_AllCategories'
)
,cte2
as
(
SELECT [attribute_name]
		,B.name
      ,[keep_attribute]
      ,[attribute_name_use]
      ,[sort_order]
	  ,Case When [attribute_name] = 'fcc' Then ',fcc' 
	  when quotename(name) is null then ',Null'
	  Else ',' + quotename([attribute_name]) End 
	  + ' As ' + quotename([attribute_name_use]) as view_columns
	  ,row_number() over(partition by attribute_name_use order by name desc) as rw
  FROM [dbo].[pbi_Product_Attributes_Master] A
  Left outer join
  cte B
  on A.attribute_name = B.name
  Where A.keep_attribute = 1
  )

  Select view_columns
  , isnull(name,'') as client_columns
  , sort_order
  , case when name is not null or [attribute_name] = 'fcc' then 1 else 0 end as column_display
  from cte2
  Where rw = 1
  order by sort_order;

```

## Brick Dimension Table

The Brick dimension table is designed to take care of all potential brick structure which is above IQVIA retail brick. There are currently potentially 3 different brick structure could be implemented:

* Retail Brick: can be used for Sellin data, typically around 1040 bricks
* Scan Brick: this is the new brick structure still in developing and evaluation. This structure is meant to be used for Scanout data with the lowest reportable geo-spatial structure. Ideally, the retail brick should be rolled up to scan brick.
* CHW brick: this is the brick structure to protect the chemist warehouse store privacy. Retail bricks are rolled up to CWH Bricks. Ideally, the scan brick can be also rolled up to CWH brick

In the ideal scenario, the brick structure is hierarchical from retail brick to scan brick to CWH brick. For the future prove and in the case of these structure is not hierarchical, a `brick_structure_base` attribute is added into the Brick dimension table. The brick structure base can be: retail brick, scan brick, or CWH brick. If the fact data is available, it can be extended to other geo-structure base as well, like ABS LGA, SA2, SA3 etc.

The Table \@ref(tab:sqbrick) below depicts the attributes used for the `dim_brick` table.

```{sql sqbrick, connection = con_odbc, echo = FALSE, max.print = -1, tab.cap = "Brick Dimension Table Columns"}

SELECT [Brick_Attributes_Name]
      ,IsNull([Description], '') As [Description]
FROM [dbo].[pbi_Brick_Attributes];

```


### Ideal Brick Rollup Structure

Ideally, all brick structure used should be roll up to any client territory. When a brick structure is in line with a client territory structure it is most valuable for client reporting purpose. Figure 
@ref(fig:brickhr) depicts the ideal brick rollup hierarchy structure. 

```{r brickhr, fig.cap="IQVIA Brick Hierarchy Structure Options", fig.height=8, fig.width=8}
library(DiagrammeR)
mermaid("
    graph TD
    RB[Retail Brick]-->SB[Scan Brick]
    SB-->CB[CWH Brick]
    CB--Option 1: Ideal Structure <p> may not practical-->CT(Client Territory)
    RB-.Option 4:<p>Retail Brick to Territory.->CT
    SB-.Option 3:<p>RB to SB to Territory.->CT
    SB1[Scan Brick.] -.Option 2:<p>Scan Brock to Territory.->CT
    CT-->SG[State Group]
    SG-->AU((National))
    style RB fill:#00b300,stroke:#333,stroke-width:4px;
    style SB fill:#00ffff,stroke:#333,stroke-width:2px;
    style SB1 fill:#00ffff,stroke:#333,stroke-width:4px;
    style CB fill:#99ccff,stroke:#333,stroke-width:2px;
    style CT fill:#f96,color:#fff,stroke:#333,stroke-width:2px;
    style SG fill:#99cc00,stroke:#333,stroke-width:2px;
    style AU fill:#F1EA0E,stroke:#333,stroke-width:4px;
    ", height = 700, width = 700)
```

Unfortunately, this was not always the case for our current CWH bricks to client territories. 

### CWH Brick as the Lowest Granular Structure

Since currently the lowest reportable scanout geography which has been evaluated by stats office is CWH brick, the Figure \@ref(fig:cwbrickhr) will be implemented. Within the each CWH brick only 2 banner groups will be reported:

* Chemist warehouse banner group: Chemist warehouse, My Chemist, Beauty spot
* Other Banners: All other banners other than CWH group

At the state level, all state reportable banners can be reported. 

```{r cwbrickhr, fig.cap="Current implementation of CWH Brick to State Group", fig.height=8, fig.width=8}
library(DiagrammeR)

mermaid("
    graph LR
    CB[CWH Brick]-->SG[State Group]
    SG-->AU((National))
    subgraph Brick_Reportable_Banners
    B1[CWH Group]
    B2[All Other Banners]
    end
    subgraph State_Reportable_Banners
    RB[All Reportable Banners]
    end
    CB-.->B1
    CB-.->B2
    SG-.->RB
    style CB fill:#99ccff,stroke:#333,stroke-width:4px;
    style SG fill:#99cc00,stroke:#333,stroke-width:2px;
    style AU fill:#F1EA0E,stroke:#333,stroke-width:4px;
    ", height = 350, width = 700)

```

With this implementation, The Brick dimension key: Brick_ID, will be same as CWH_Brick_ID. Brick_Name, Scan_Brick_ID, and Scan_Brick_Name will be blank or null value.

## Chanel Dimension Table

Channel dimension table is a very simple table to distinguish if a banner is Retail or Grocery. 


## Banner Dimension Table

Banner Dimension has a `channel` attribute to distinguish: retail pharmacy and grocery. DS team has added some additional attributes into the Banner dimension table. These attributes can be reviewed and incorporate into BDF master table. `Is_Banner_Client_Reportable` refers to client subscription to specific banners. This attributes is only useful for client not useful for the internal user. The attributes therefore is client specific. For simplicity, there is no link between banner to client module. The attributes value is populated during the client ETL stage.

A certain banners, IQVIA doesn't have enough panel stores to make the banner reportable. These banners include:

* Priceline
  + Priceline Pharmacy
  + Priceline Health & Beauty
* Good Price Pharmacy
* National Pharmacies

For these banners, if a client bought the data from other source then we can integrate them into our data and make these banner reportable. The last 3 invisible attributes are used to indicate if such banner is reportable.

The Table \@ref(tab:sqbanner) below depicts the attributes used for the `dim_banner` table.

```{sql sqbanner, connection = con_odbc, echo = FALSE, max.print = -1, tab.cap = "Banner Dimension Table Columns"}

SELECT [banner_Attributes]
      ,[Description]
      ,[Client_Display]
FROM [dbo].[pbi_Banner_Attributes];

```

Given that, not all banners are reportable at a specific state level, from banner to fact table there is a transition dimension table banner state.

When there is not enough panel stores in a state for a reportable banner at a particular period, the state will group to Other States. Since our Scanout data projection has made a major overhaul. The new projection can project data down to Store + FCC level. 

### Banner State Dimension Table

Banner state Dimmesion table is used for client for banner reporting. Due to a reportable banner state changes based on the different time period, to report the banner state related sales data, the banner state attributes in this table have to be used. **_This means a combined attributes from Banner table and State table, will not yield the same results as the same combination in this table!_**

```{sql sqbannerS, connection = con_odbc, echo = FALSE, max.print = -1, tab.cap = "Banner State Dimension Table Columns"}

SELECT [Banner_State_Attributes]
      ,IsNull([Description], '') As [Description]
      ,[Client_Display]
FROM [dbo].[pbi_Banner_State_Attributes];

```

Due to the complexity of IRI pannel banner data, like Priceline & Good price pharmacy. Some SKUs which is calculated in IQVIA projected sales data may not included in the IRI supplied data for a particular client for various reasons, like an SKU is not included in a client's market definition. For this reason, those SKUs which are not included in IRI data based on a client definition will need to assign to Other banner Other states for the overall sales completeness. we created a dummy banner state code to hold this data. At the reporting stage, based on the reportable banner state attributes, it will all go to the Other banner Other states. 

This dummy record is illustrated as in the table \@ref(tab:btdummy) below:

```{sql sqbtdummy, connection = con_odbc_iqvia, echo = FALSE, max.print = -1, output.var="d_btdummy"}

SELECT *
FROM [dbo].[dim_banner_state]
Where banner_state_id = -999;
```

```{r btdummy}

df <- t(d_btdummy) %>% data.frame(stringsAsFactors = F)

names <- rownames(df)
rownames(df) <- NULL
df <- cbind(names, df)

names(df) <- c("Attribute_Name", "Date_Value_Example")

# knitr::kable(df, caption = "Dim Banner State Dummy Code")
df %>% 
 mutate(
    Date_Value_Example = cell_spec(Date_Value_Example, "html", color = ifelse(Attribute_Name == "banner_state_id", "red", "black"))
  ) %>%
  kable(format = "html", escape = F, caption = "Dim Banner State Dummy Code") %>%
    footnote(general = "The integer -999 is a hardcoded number to capture all sales records which are not presented in IRI datasets.",
           fixed_small_size = TRUE
           )

```


## Date(Period) Dimension Table

Date dimension table is a key dimension table for all reporting, as most sales reporting needs time intelligence.

In this data model, monthly and weekly fact data will be combined in the same fact table. Therefore, the monthly and weekly date will be combined in the same dimension table.

For a client, when a period name is not specified and there is no date attributes included, a default period name will be used to display measures at the latest date. The default period name is a configurable value for each client, which can be one of all available period names. This means one client may use MAT while the another may use MQT.

The Figure \@ref(fig:ptype) below depicts Date dimension table period types and all available period names and its rollup path.

```{r ptype, layout="l-screen-insert", fig.cap="Date Period Type and its Rollup Hierarchy"}

knitr::include_graphics("image/period_type.jpg")

```

### Period Type and Period Name Configuration Table

A global period name configuration table will add into our data model data warehouse to centrally control the available period name and its periods covered as in the table below:

This table will be used as a reference table to generate the Date Dimension table attributes as described in the section of Date Dimension table. Table \@ref(tab:pconfig) depicts the currently used period names and its configuration values.

```{sql sqpconfig, connection = con_odbc_iqvia, echo = FALSE, max.print = -1, output.var="p_config"}

SELECT [Period_Type]
      ,[Period_Name]
      ,[Periods]
      ,[Period_Short_Name]
FROM [dbo].[Admin_Config_Period_All];
```

```{r pconfig}

p_config %>%
  mutate(
    Period_Name = cell_spec(Period_Name, "html", color = ifelse(Periods < 0, "red", "darkblue")),
    Periods = cell_spec(Periods, "html", color = ifelse(Periods < 0, "red", "darkblue"))
  ) %>%
  kable(format = "html", escape = F, caption = "Period Name Configuration Table") %>% 
    footnote(general = "Periods attribute is the number of reoccuring periods covered in a period name. ",
           number = c("Negative value means there is really no fixed periods; ", "YTD, Calendar Year, Calendar Quarter, YTD Weeks change number of periods at each diffrerent date so using negative values; ",
                      "There is no particular meaning for the value of negative pe se it is just used to distinguish period names;"),
           fixed_small_size = TRUE
           )

```


Given that the different period names has different descriptive names and other attributes values, two date dimension tables will be created:

* Date_Base dimension table: this one is directly linked to Fact table to form a one to many relationship. The dimension table is invisible to end users.
* Date Dimension table: based on the hierarchy structure depicted in the figure \@ref(fig:ptype), the lowest date unit weekly or monthly will be replicated in many period name hierarchy. The Date dimension table and Date_Base table is many to one relationship.


### Date Dimension Key Consideration


It is a good practice to use a meaningful key as data surrogate key, this can reduce the time to infer the meaning of date key in the fact table. The data surrogate key is integer data type, we define the data surrogate key as below convention:

__`YYYYMMDDXZ`__

Where the last 2 digits X and Z are period Starting/Ending and period type indicator:

* For X
    + if the date is using a period starting date, then X = 0;
    + if the date is using a period ending date, then X = 1
* For Z
    + if the period type is `Monthly`, then Z = 1;
    + if the period type is `Weekly`, then Z = 2

Examples:

* 2018050101 this stands for the date if a monthly date, and using the month start date $1^{st} May\:2018$;
* 2020071812 this stands for the date is a weekly date, and using the week end date $18^{th} July\:2020$


### Date Base Table

Date base dimension table is very simple and it only has 4 columns. The table columns and its value examples is as in Table \@ref(tab:dbase) as below:

```{sql sqDbase, connection = con_odbc_iqvia, echo = FALSE, max.print = -1, output.var="d_base"}

SELECT top (1)[period_id]
      ,[period_base]
      ,[period_start]
      ,[period_end]
FROM [dbo].[dim_date_base];
```

```{r dbase}
df <- t(d_base) %>% data.frame()

names <- rownames(df)
rownames(df) <- NULL
df <- cbind(names,df)

names(df) <- c("Attribute Name", "Date Value Example")

knitr::kable(df, caption = "Date Base Dimension Table Columns")

```


### Date Dimension Table {#CDimDate}


Date dimension table columns and its value examples is as in Table \@ref(tab:ddim) as below:

```{sql sqDDim, connection = con_odbc_iqvia, echo = FALSE, max.print = -1, output.var="d_date"}

SELECT TOP (3) *
FROM [dbo].[vw_Dim_Date]
Order by newid();
```

```{r ddim}
df <- t(d_date) %>% data.frame()

names <- rownames(df)
rownames(df) <- NULL
df <- cbind(names, df)

names(df) <- c("Attribute Name", "Value Example 1", "Value Example 2", "Value Example 3")

knitr::kable(df, caption = "Date Dimension Table Columns")

```

### Client Discrete Date Reporting

Some clients have a set of special dates or frequencies to be reported. There are a couple of ways to implement this. The actual implementation will be added later.


## Sellin_Scanout Dimension Table

This is a very simple dimension table to separate the fact data into Sellin/Scanout, Monthly/Weekly, and Calendar Year partition combinations. 

To separate the Sellin/Scanout, Monthly/Weekly, and Calendar Year combinations into different partitions in the fact table renders the different frequency of updates of the fact data possible. This means we can update sellin/scanout and monthly/weekly data in the same fact table at the different cadence without reloading everything. This can save a significant data reloading time and make the Power BI service incremental refreshing possible.


## Store Dimension Table

The store dimension table is built for calculating the store distribution metrics and also for the internal data analysis use. This dimension table will be visible for internal users but it will be hidden for the external clients. However, if a client is subscribed to IQVIA store level data, then this table can be set to visible to a client as well.

DS team has made effort to include many ABS geospatial attributes namely:

- LGA, local government area;
- SUA, significant urban area;
- GCCSA, Greater Capital City Statistical Area;
- SA1, ABS statistical area level 1
- SA2, ABS statistical area level 2
- SA3, ABS statistical area level 3
- SA4, ABS statistical area level 4


It will be great that these attributes can also be added into the Outlet master table. 

Store dimension table columns and its value examples is as in Table \@ref(tab:dstore) as below:

```{sql sqDStore, connection = con_odbc_iqvia, echo = FALSE, max.print = -1, output.var="d_store"}

SELECT TOP (2) *
FROM [dbo].[dim_store]
Order by newid();
```

```{r dstore}
df <- t(d_store) %>% data.frame()

names <- rownames(df)
rownames(df) <- NULL
df <- cbind(names, df)

names(df) <- c("Attribute Name", "Value Example 1", "Value Example 2")

knitr::kable(df, caption = "Store Dimension Table Columns")

```

### Grocery Store Consideration

While we currently don't have the grocery store level information, the store table is designed to incorporate the grocery store as well. However, there is no such information currently.


## Store Banner Period Table

Since a store's banner association is time related, a store may change its banner association over time. The dim_store_banner_period dimension table is a transition table to hold such information. It is also a semi-fact table as it can count the banner and/or banner state store members over time. Three key is sufficient to populate this table. However, for the pre-Power BI load ETL process purpose, there are additional attributes included in this table for look-up dimension key purpose.


```{sql sqsbanner, connection = con_odbc, echo = FALSE, max.print = -1, tab.cap = "Store Banner State Dimension Table Columns"}

SELECT [Store_Banner_Attributes]
      ,[Description]
      ,[Client_Display]
FROM [dbo].[pbi_Store_Banner_State];

```


This the example record of dim_store_banner_state table is illustrated as in the table \@ref(tab:storebs) below:

```{sql sqstorebs, connection = con_odbc_iqvia, echo = FALSE, max.print = -1, output.var="d_storebs"}

SELECT top (1) *
FROM [dbo].[dim_store_banner_state];
```

```{r storebs}

df <- t(d_storebs) %>% data.frame(stringsAsFactors = F)

names <- rownames(df)
rownames(df) <- NULL
df <- cbind(names, df)

names(df) <- c("Attribute_Name", "Date_Value_Example")

knitr::kable(df, caption = "Dim Store Banner State REcord Example")
# df %>% 
#  mutate(
#     Date_Value_Example = cell_spec(Date_Value_Example, "html", color = ifelse(Attribute_Name == "banner_state_id", "red", "black"))
#   ) %>%
#   kable(format = "html", escape = F, caption = "Dim Banner State Dummy Code") %>%
#     footnote(general = "The integer -999 is a hardcoded number to capture all sales records which are not presented in IRI datasets.",
#            fixed_small_size = TRUE
#            )

```



# Fact Table Design Considerations

These Semantic data model only have 3 fact tables:

1. fact_Brick_Banner_Sales: this fact table stores the brick banner lever of sales data. The sales data is loaded with the below configuration:
    + Row-wise:
        + Monthly
        + Weekly
        + Sellin
        + Scanout
        + Channel
    + Column-wise:
        + Amount
        + Unit
        + Baseline Amount
        + Baseline Unit
        + Promotion Amount
        + Promotion Unit
        + Export Amount
        + Export Unit
        + Online Amount (Will not be reported at brick level but state or national level. CWH is separable At national level)
        + Online Unit (Will not be reported at brick level but state or national level. CWH is separable At national level)
        
2. fact_store_sales: this fact table stores the store lever of sales data. The sales data is loaded with the below configuration:
    + Row-wise:
        + Monthly
        + Weekly
        + Sellin
        + Scanout
        + Channel
    + Column-wise:
        + Amount
        + Unit
        + Baseline Amount
        + Baseline Unit
        + Promotion Amount
        + Promotion Unit
        + Export Amount
        + Export Unit
    + Online sales will need to create dummy store to hold all Non-CWH online sales at state level
        
    
3. fact_iri_grocery_distribuion: more info to be added later


## Storage File Group Consideration

```{sql sqlFilegroup, connection = con_odbc_iqvia, echo = TRUE, eval = FALSE}

--create a new database file group
ALTER DATABASE [iqviaau_reporting_db] ADD FILEGROUP [fg_Fact_Data];
ALTER DATABASE [iqviaau_reporting_db] ADD FILEGROUP [fg_Fact_Store];


--Add new data file for fact brick sales to the new file group
ALTER DATABASE [iqviaau_reporting_db]
    ADD FILE 
    (
    NAME = [iqviaau_reporting_db_fb],
    FILENAME = 'd:\Program Files\Microsoft SQL Server\MSSQL13.MSSQLSERVER\MSSQL\DATA\iqviaau_reporting_db_fact_fb.ndf',
        SIZE = 5120 MB, 
        MAXSIZE = UNLIMITED, 
        FILEGROWTH = 1024 MB
    ) TO FILEGROUP [fg_Fact_Data];

--Add new data file for fact brick sales to the new file group
ALTER DATABASE [iqviaau_reporting_db]
    ADD FILE 
    (
    NAME = [iqviaau_reporting_db_fs],
    FILENAME = 'd:\Program Files\Microsoft SQL Server\MSSQL13.MSSQLSERVER\MSSQL\DATA\iqviaau_reporting_db_fact_fs.ndf',
        SIZE = 5120 MB, 
        MAXSIZE = UNLIMITED, 
        FILEGROWTH = 1024 MB
    ) TO FILEGROUP [fg_Fact_Store];

-- create table partition function

CREATE PARTITION FUNCTION [func_salesPart](int) 
AS RANGE LEFT FOR VALUES (101, 102, 103, 104, 105, 106, 201, 202, 203, 204, 205, 206, 301, 302, 303, 304, 305, 401, 402, 403, 404, 405)

--Create Partition scheme
CREATE PARTITION SCHEME [sch_brickSalesPart] AS PARTITION [func_salesPart] All TO ([fg_Fact_Data]);
CREATE PARTITION SCHEME [sch_storeSalesPart] AS PARTITION [func_salesPart] All TO ([fg_Fact_Store]);

```

## Brick Banner Level Sales Fact Table


### Table partition

```{sql sqlFactBrick, connection = con_odbc_iqvia, echo = TRUE, eval = FALSE}

-- Create fact banner brick sales table for non-IRI panel banners (IQVIA Banner only)
CREATE TABLE [dbo].[fact_banner_brick_sales_nip] (
	[partition_key] [int] Not Null,
	[banner_state_id] [int] Not Null,
	[brick_id] [int] not Null,
	[period_id] [int] Not NULL,
	[fcc] [int] Not NULL,
	[amount] [bigint] NULL,
	[unit] [bigint] NULL,
	[baseline_sales_amount] [bigint] NULL,
	[baseline_sales_unit] [bigint] NULL,
	[promotion_sales_amount] [bigint] NULL,
	[promotion_sales_unit] [bigint] NULL,
	[export_amount] [bigint] NULL,
	[export_unit] [bigint] NULL,
	[online_amount] [bigint] NULL,
	[online_unit] [bigint] NULL
) ON [sch_brickSalesPart] (partition_key);

go

CREATE CLUSTERED COLUMNSTORE INDEX [cci_fact_banner_brick_sales_nip] ON [dbo].[fact_banner_brick_sales_nip] 
WITH (DROP_EXISTING = OFF, COMPRESSION_DELAY = 0) ON [sch_brickSalesPart] (partition_key);

go
-- Create fact banner brick sales staging table for non-IRI panel banners (IQVIA Banner only)
CREATE TABLE [dbo].[fact_banner_brick_sales_nip_stage] (
	[partition_key] [int] Not Null,
	[banner_state_id] [int] Not Null,
	[brick_id] [int] not Null,
	[period_id] [int] Not NULL,
	[fcc] [int] Not NULL,
	[amount] [bigint] NULL,
	[unit] [bigint] NULL,
	[baseline_sales_amount] [bigint] NULL,
	[baseline_sales_unit] [bigint] NULL,
	[promotion_sales_amount] [bigint] NULL,
	[promotion_sales_unit] [bigint] NULL,
	[export_amount] [bigint] NULL,
	[export_unit] [bigint] NULL,
	[online_amount] [bigint] NULL,
	[online_unit] [bigint] NULL
) ON [fg_Fact_Data];

GO

ALTER TABLE [dbo].[fact_banner_brick_sales_nip_stage]  WITH CHECK ADD  CONSTRAINT [chkBBPartID] CHECK  (partition_key > 100 AND partition_key <= 101);

GO

ALTER TABLE [dbo].[fact_banner_brick_sales_nip_stage] CHECK CONSTRAINT [chkBBPartID];

GO

CREATE CLUSTERED COLUMNSTORE INDEX [cci_fact_banner_brick_sales_nip_stage] ON [dbo].[fact_banner_brick_sales_nip_stage] 
WITH (DROP_EXISTING = OFF, COMPRESSION_DELAY = 0) ON [fg_Fact_Data];

--ALTER TABLE [dbo].[fact_banner_brick_sales_nip_stage]  DROP CONSTRAINT if exists [chkBBPartID];
--DROP INDEX IF Exists [cci_fact_banner_brick_sales_nip_stage] ON [dbo].[fact_banner_brick_sales_nip_stage];

```

### Partition Data Switch In/Out

```{sql sqlrefreshFactBrick, connection = con_odbc_iqvia, echo = TRUE, eval = FALSE}
Create Proc [dbo].[usp_refresh_fact_banner_brick_sales_nip_SI_MTH]
@year as int
As
/*
This proc was created by Steven Wang at 31 July 2020
nip: means non-IRI pannel banners, which means if the data if from IRI, then they are not included
SI_MTH: Sellin data Monthly
*/
set nocount on;

--Declare @year int;
--Set @year = 2019;

Declare @part_key int;
Declare @part_id int;
Declare @period_start int;
Declare @period_end int;
Declare @sql_ch varchar(max);
Declare @sql varchar(max);
Declare @sql_out varchar(max);
Declare @date_updated date;
set @date_updated = cast(getdate() as date)
print @date_updated

Set @period_start = @year * 1000000 + 10101
print @period_start

Set @period_end = (@year + 1) * 1000000 + 10101
print @period_end

SELECT @part_key = [partition_key]
, @part_id  = [part_id]
FROM [dbo].[vw_Dim_Sellin_Scanout]
Where [sales_type] = 'Sellin'
and [period_type] = 'Monthly'
And [period_year] = @year;

print @part_key
print @part_id

If @part_key = 101 
	set @sql_ch = '
			ALTER TABLE [dbo].[fact_banner_brick_sales_nip_stage]  
			WITH CHECK ADD  CONSTRAINT [chkBBPartID] 
			CHECK  (partition_key IS NOT NULL AND partition_key <= $$);'
Else 
	set @sql_ch = '
				ALTER TABLE [dbo].[fact_banner_brick_sales_nip_stage]
				WITH CHECK ADD CONSTRAINT [chkBBPartID]
				CHECK (partition_key > $$ - 1 AND partition_key <= $$);'
print @sql_ch

set @sql = 'ALTER TABLE [dbo].[fact_banner_brick_sales_nip_stage] SWITCH TO [dbo].[fact_banner_brick_sales_nip] PARTITION $$;'
set @sql_out = 'ALTER TABLE [dbo].[fact_banner_brick_sales_nip] SWITCH PARTITION $$ TO [dbo].[fact_banner_brick_sales_nip_stage];'

--delete all the data in staging table
Truncate Table fact_banner_brick_sales_nip_stage;
--drop constraint
ALTER TABLE [dbo].[fact_banner_brick_sales_nip_stage]  DROP CONSTRAINT if exists [chkBBPartID];
--create the columnstore index to match with the fact table
if not exists (Select 1 from sys.indexes Where name = 'cci_fact_banner_brick_sales_nip_stage')
	begin
		CREATE CLUSTERED COLUMNSTORE INDEX [cci_fact_banner_brick_sales_nip_stage] ON [dbo].[fact_banner_brick_sales_nip_stage] 
		WITH (DROP_EXISTING = OFF, COMPRESSION_DELAY = 0) ON [fg_Fact_Data];
	end
--performa the swith out
set @sql_out = replace(@sql_out, '$$', @part_id);
exec(@sql_out);

--delete the switched out data
Truncate Table fact_banner_brick_sales_nip_stage;
--drop the columnstore index for faster insert
DROP INDEX IF Exists [cci_fact_banner_brick_sales_nip_stage] ON [dbo].[fact_banner_brick_sales_nip_stage];

--Insert updated data to stage
insert into [dbo].[fact_banner_brick_sales_nip_stage] with(tablock)
([partition_key]
,[banner_state_id]
,[brick_id]
,[period_id]
,[fcc]
,[amount]
,[unit]
,[baseline_sales_amount]
,[baseline_sales_unit]
,[promotion_sales_amount]
,[promotion_sales_unit]
,[export_amount]
,[export_unit]
,[online_amount]
,[online_unit]
,[date_updated])
select
@part_key As partition_key
, b.[banner_state_id]
,b.[brick_id]
,a.Period_Id
,a.fcc
,cast(round(sum(a.Amount)*100,0) as bigint) as Amount
,cast(round(sum(a.Unit)*100,0) as bigint) as Unit
,0 as [baseline_sales_amount]
,0 as [baseline_sales_unit]
,0 as [promotion_sales_amount]
,0 as [promotion_sales_unit]
,0 as [export_amount]
,0 as [export_unit]
,0 as [online_amount]
,0 as [online_unit]
,@date_updated As [date_updated]
from [dbo].[ddd_monthly_sales] as a
inner join
[dbo].[dim_store_banner_state] as b
on a.Outlet=b.[outlet]
and a.Period_Id=b.[period_id]
Where 
a.Period_Id >= @period_start
and a.Period_Id < @period_end
group by 
b.[banner_state_id]
,b.[brick_id], a.Period_Id, a.fcc;

if not exists (Select 1 from sys.indexes Where name = 'cci_fact_banner_brick_sales_nip_stage')
	begin
		CREATE CLUSTERED COLUMNSTORE INDEX [cci_fact_banner_brick_sales_nip_stage] ON [dbo].[fact_banner_brick_sales_nip_stage] 
		WITH (DROP_EXISTING = OFF, COMPRESSION_DELAY = 0) ON [fg_Fact_Data];
	end

--check the data integrity
set @sql_ch = replace(@sql_ch, '$$', @part_key);
exec(@sql_ch);

--perform the data swith in
set @sql = replace(@sql, '$$', @part_id);
exec(@sql);

return 0;

```


### Data Refresh Strategy


```{sql sqlrefreshall, connection = con_odbc_iqvia, echo = TRUE, eval = FALSE}
CREATE Proc [dbo].[usp_refresh_fact_banner_brick_sales_ALL]
@banner_panel varchar(20) = 'nip', -- 'nip', 'ipo_iqvia', 'ipo_iri'
@data_source varchar(20) = 'Scanout', -- '%', 'Sellin', 'Scanout'
@period_type varchar(20) = 'Weekly', -- '%', 'Monthly', 'Weekly'
@year as varchar(4) = '%' -- 2018
As
/*
This proc was created by Steven Wang at 31 July 2020
nip: means non-IRI pannel banners, which means if the data if from IRI, then they are not included
All: means to refresh all partitions

to run the stored procedure use:

EXEC	[dbo].[usp_refresh_fact_banner_brick_sales_ALL]
		@banner_panel = N'nip', --available values: 'nip', 'ipo_iqvia', 'ipo_iri'
		@data_source = N'%', -- available values: '%', 'Sellin', 'Scanout'
		@period_type = N'%', -- available values: '%', 'Monthly', 'Weekly'
		@year = N'%'; -- available values: '%', '2020', '2019', etc
*/
set nocount on;

--Declare @year int;
--Set @year = 2019;

Declare @data_source_in varchar(20);
Declare @period_type_in varchar(20);
Declare @year_in int;
Declare @SO_Table As table (
	[sales_type] varchar(20), 
	[period_type] varchar(20), 
	[period_year] int,
	part_id int)

insert into @SO_Table
Select
[sales_type],
[period_type],
[period_year],
part_id
FROM [dbo].[vw_Dim_Sellin_Scanout]
Where [sales_type] like @data_source
and [period_type] like @period_type
And cast([period_year] as varchar(4)) like @year


SELECT top (1)
@data_source_in = [sales_type]
, @period_type_in = [period_type]
, @year_in = [period_year]
FROM @SO_Table
order by part_id;

--print @data_source_in
--print @period_type_in
--print @year_in

While 0 = 0
Begin
	
	if @banner_panel = 'nip' and @data_source_in = 'Sellin' And @period_type_in = 'Monthly'
		Begin
			EXEC [dbo].[usp_refresh_fact_banner_brick_sales_nip_SI_MTH] @year = @year_in
		end

	if @banner_panel = 'nip' and @data_source_in = 'Sellin' And @period_type_in = 'Weekly'
		Begin
			EXEC [dbo].[usp_refresh_fact_banner_brick_sales_nip_SI_WK] @year = @year_in
		end	

	if @banner_panel = 'nip' and @data_source_in = 'Scanout' And @period_type_in = 'Monthly'
		Begin
			EXEC [dbo].[usp_refresh_fact_banner_brick_sales_nip_SO_MTH] @year = @year_in
		end

	if @banner_panel = 'nip' and @data_source_in = 'Scanout' And @period_type_in = 'Weekly'
		Begin
			EXEC [dbo].[usp_refresh_fact_banner_brick_sales_nip_SO_WK] @year = @year_in
		end

	if @banner_panel = 'ipo_iqvia' and @data_source_in = 'Scanout' And @period_type_in = 'Monthly'
		Begin
			EXEC [dbo].[usp_refresh_fact_banner_brick_sales_ipo_iqvia_SO_MTH] @year = @year_in
		end

	if @banner_panel = 'ipo_iqvia' and @data_source_in = 'Scanout' And @period_type_in = 'Weekly'
		Begin
			EXEC [dbo].[usp_refresh_fact_banner_brick_sales_ipo_iqvia_SO_WK] @year = @year_in
		end


	Delete FROM @SO_Table
	Where [sales_type] = @data_source_in
	And [period_type] = @period_type_in
	And [period_year] = @year_in;

	SELECT top (1)
	@data_source_in = [sales_type]
	, @period_type_in = [period_type]
	, @year_in = [period_year]
	FROM @SO_Table
	order by part_id;

	if @@ROWCOUNT = 0
		Break
end

return 0;
```


partitions:

1. sellin weekly
2. sellin monthly
3. scanout weekly
4. scanout monthly


### Scanout Sales Data

### Sellin Sales Data

### Online Sales Data

### Domestic/Exporting Sales Splitting

### Grocery Sales Data

### IRI Sales Data

### CBG Sales Data Consideration

## Store Level Sales Fact Table

### Grocery Store Level Sales Reverse Engineering from Aggregated Grocery Data

### IRI Store Level Sales Reverse Engineering from Aggregated Grocery Data


# Power BI Master Template Design

## Power BI Master Data Model

## Template Parameter Design

## Fact Table Data Partition and Incremental Data Refreshing



FactBanner_Brick_PL
FactBanner_Brick_GP
FactBanner_Brick_NP

1.get banner_state, period, exporting_ratio,and first birckID, from the fact baner _brick sales, table 1


2. cartisan join IRI data by:
  banner state
  period
  fcc
  
3 inerjoin table 1 on banenr state id, period

 banner state
  period
  fcc
  brick_Id

4. left join with IRI datam, fill all null value with 0

5. full join with this banner's data with IQVIA data, where peiroed>= '2020-01-01'

  1. if iri data is NULL replace banner state id with otherbaner, other state id, then use the iqvia value as vlaue
  2. if sum(iri_value) over(parition by banner_state_id, period, fcc) > 0, sum(iri_value) over(parition by banner_state_id, period, fcc) / count(banner_state_id) over(parition by banner_state_id, period, fcc)\use this value to replace IQIVA value


6. use IQVIA data left join IRI_product_Master_client, where peiroed < '2020-01-01'
1. if iri FCC is NULL replace banner state id with other baner, other state id, then use the iqvia value as vlaue
2. OTHERWIse no change




  fact_banner_brick_sales_ipo_iqivia
  fact_banner_brick_sales_ipo_iri
  fact_banner_brick_sales_ipo_combined

  fact_banner_brick_sales_nip  (no iri banner pannel)
  Union All
  fact_banner_brick_sales_ipo_combined  (iri pannel)
   
  xyz_dim_Banner
  xyz_fact_Banner_Brick_Sales
  

```{r}
# 
# library(htmltools)
# library(flextable)
# ft <- flextable(head(iris))
# tab_list <- list()
# for(i in 1:3){
#   tab_list[[i]] <- tagList(
#     tags$h3(paste0("iteration ", i)),
#     tags$p("JUst for the test pupose"),
#     htmltools_value(ft)
#   )
# }
# tagList(tab_list)
# 
```














